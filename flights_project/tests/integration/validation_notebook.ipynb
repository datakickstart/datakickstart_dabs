{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Validation notebook\n",
    "\n",
    "This notebook is executed using Databricks Workflows as defined in resources/notebook_validation_job.yml. It is used to check summary table for valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame assert\n",
    "Compare results from test data set against an expected set of values that is generated with simpler logic. This is more dynamic but involves putting more logic into the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "result_counts = spark.sql(\"\"\"\n",
    "        SELECT sum(case when deptime is null then 1 else 0 end) null_deptime_count, count(1) rows\n",
    "        FROM main.flights_test.flights_raw\n",
    "        \"\"\")\n",
    "\n",
    "csv_schema = schema = StructType([\n",
    "      StructField(\"Year\", IntegerType(), True),\n",
    "      StructField(\"Month\", IntegerType(), True),\n",
    "      StructField(\"DayofMonth\", IntegerType(), True),\n",
    "      StructField(\"DayOfWeek\", IntegerType(), True),\n",
    "      StructField(\"DepTime\", StringType(), True),\n",
    "      StructField(\"CRSDepTime\", IntegerType(), True),\n",
    "      StructField(\"ArrTime\", StringType(), True),\n",
    "      StructField(\"CRSArrTime\", IntegerType(), True),\n",
    "      StructField(\"UniqueCarrier\", StringType(), True),\n",
    "      StructField(\"FlightNum\", IntegerType(), True),\n",
    "      StructField(\"TailNum\", StringType(), True),\n",
    "      StructField(\"ActualElapsedTime\", StringType(), True),\n",
    "      StructField(\"CRSElapsedTime\", IntegerType(), True),\n",
    "      StructField(\"AirTime\", StringType(), True),\n",
    "      StructField(\"ArrDelay\", StringType(), True),\n",
    "      StructField(\"DepDelay\", StringType(), True),\n",
    "      StructField(\"Origin\", StringType(), True),\n",
    "      StructField(\"Dest\", StringType(), True),\n",
    "      StructField(\"Distance\", StringType(), True),\n",
    "      StructField(\"TaxiIn\", StringType(), True),\n",
    "      StructField(\"TaxiOut\", StringType(), True),\n",
    "      StructField(\"Cancelled\", IntegerType(), True),\n",
    "      StructField(\"CancellationCode\", StringType(), True),\n",
    "      StructField(\"Diverted\", IntegerType(), True),\n",
    "      StructField(\"CarrierDelay\", StringType(), True),\n",
    "      StructField(\"WeatherDelay\", StringType(), True),\n",
    "      StructField(\"NASDelay\", StringType(), True),\n",
    "      StructField(\"SecurityDelay\", StringType(), True),\n",
    "      StructField(\"LateAircraftDelay\", StringType(), True),\n",
    "      StructField(\"IsArrDelayed\", StringType(), True),\n",
    "      StructField(\"IsDepDelayed\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "expected_df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"false\")\n",
    "      .schema(csv_schema)\n",
    "      .load(\"/databricks-datasets/airlines/\")\n",
    "      .limit(1000)\n",
    "    )\n",
    "expected_df.createOrReplaceTempView(\"expected_flights_raw\")\n",
    "expected_counts = spark.sql(\"\"\"\n",
    "        SELECT sum(case when deptime is null then 1 else 0 end) null_deptime_count, count(1) rows\n",
    "        FROM expected_flights_raw\n",
    "        \"\"\")\n",
    "\n",
    "assertDataFrameEqual(result_counts, expected_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b174dd1e-3c3d-4933-a4c4-8e5d52500cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple assert\n",
    "Option you can use if counts will stay consistent in the test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "        SELECT count(distinct deptime) deptime_count, count(1) rows\n",
    "        FROM main.flights_test.flights_raw\n",
    "        \"\"\").first()\n",
    "\n",
    "# Option 1\n",
    "# assert result.dt_count == 398\n",
    "assert result.rows == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No errors detected\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
