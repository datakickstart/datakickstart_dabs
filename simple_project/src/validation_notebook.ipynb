{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Validation notebook\n",
    "\n",
    "This notebook is executed using Databricks Workflows as defined in resources/notebook_validation_job.yml. It is used to check summary table for valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame assert\n",
    "Compare results from test data set against an expected set of values that is generated with simpler logic. This is more dynamic but involves putting more logic into the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "result_counts = spark.sql(\"\"\"\n",
    "        SELECT count(distinct pickup_date) dt_count, count(1) rows\n",
    "        FROM main.datakickstart_dev.trip_summary\n",
    "        \"\"\")\n",
    "\n",
    "expected_counts = spark.sql(\"\"\"\n",
    "        WITH source_agg (\n",
    "            SELECT cast(tpep_pickup_datetime as date) dt,\n",
    "                   pickup_zip,\n",
    "                   1 as row_count\n",
    "            FROM samples.nyctaxi.trips\n",
    "            GROUP BY dt, pickup_zip\n",
    "        )\n",
    "        SELECT count(distinct dt) dt_count, count(1) rows\n",
    "        FROM source_agg\n",
    "        \"\"\")\n",
    "\n",
    "assertDataFrameEqual(result_counts, expected_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b174dd1e-3c3d-4933-a4c4-8e5d52500cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple assert\n",
    "Option you can use if counts will stay consistent in the test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "        SELECT count(distinct pickup_date) dt_count, count(1) rows\n",
    "        FROM main.datakickstart_dev.trip_summary\n",
    "        \"\"\").first()\n",
    "\n",
    "# Option 1\n",
    "assert result.dt_count == 60\n",
    "assert result.rows == 3290\n",
    "\n",
    "# Option 2\n",
    "expected_counts = Row(dt_count=60, rows=3290)\n",
    "assert result == expected_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No errors detected\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
